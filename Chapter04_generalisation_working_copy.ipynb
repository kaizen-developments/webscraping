{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem: Parse the title and body of the websites Brookings and NYTimes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enumerate the getter functions!\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def getPage(url):\n",
    "    req = requests.get(url)\n",
    "    return BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "def getTextOfFirstTagMatchedInSoup(searchPattern, bs, property = \"tag\"):\n",
    "    try:\n",
    "        title = bs.find(**searchPattern).text\n",
    "    except AttributeError as e:\n",
    "        print(f\"An attempt was made to access the text of the assumed to be {property}, None was accessed.\")\n",
    "        return ''\n",
    "    return title\n",
    "\n",
    "def getTextOfSecondTagMatchedInSoup(searchPattern, bs, property = \"tag\"):\n",
    "    try:\n",
    "        titles = bs.find_all(**searchPattern)\n",
    "        title = titles[0].text\n",
    "    except IndexError as e:\n",
    "        print(f\"The title tags can be indexed atmost by {len(titles)-1}, current index is {1}!\")\n",
    "        return ''\n",
    "    except AttributeError as e:\n",
    "        print(f\"An attempt was made to access the text of the assumed to be {property}, None was accessed.\")\n",
    "        return ''\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enumerate the classes to be used!\n",
    "\n",
    "class Content:\n",
    "    def __init__(self, url = \"s\", title = \"s\", body = \"s\"):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "    def __str__(self):\n",
    "        return 'Title: {}'.format(self.title) + \"\\n\" + ('URL: {}\\n'.format(self.url)) + \"\\n\" + (self.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enumerate the types being used!\n",
    "from typing import Callable, Tuple, Dict, List\n",
    "\n",
    "\n",
    "Pattern = dict\n",
    "Url = str\n",
    "Title = str \n",
    "Body = str\n",
    "Strategy = Callable\n",
    "Strategies = Dict[str, Strategy]\n",
    "Patterns = Dict[str, Pattern]\n",
    "Component = str\n",
    "Domain = str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['url', 'title', 'body'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create an abstract class for checking whether the dataModel has a default value or not!\n",
    "\n",
    "\n",
    "import abc\n",
    "import inspect\n",
    "from typing import Type\n",
    "\n",
    "\n",
    "class TypeWithADefaultValue(abc.ABC):\n",
    "    def __init_subclass__(cls, **kwargs):\n",
    "        super().__init_subclass__(**kwargs)\n",
    "        cls._check_init_defaults()\n",
    "\n",
    "    @classmethod\n",
    "    def _check_init_defaults(cls):\n",
    "        init_signature = inspect.signature(cls.__init__)\n",
    "        for name, param in init_signature.parameters.items():\n",
    "            if name == 'self':\n",
    "                continue\n",
    "            if param.default is inspect.Parameter.empty:\n",
    "                raise TypeError(f\"Parameter '{name}' in {cls.__name__}.__init__ must have a default value\")\n",
    "\n",
    "Content().__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define how to obtain the components of a class with a default value!\n",
    "def componentsOf(dataModel : type): #TODO: Test whether base class has default value or not\n",
    "    # if not issubclass(dataModel, TypeWithADefaultValue):\n",
    "    #     raise TypeError(\"The dataModel does not have a default value.\")\n",
    "    return list(dataModel().__dict__.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the general function from which the scraper for each individual website will be created based on the classes to be used!\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "#TODO: DO the generalization\n",
    "def scrapeDomainForComponentsOfDataModel(url : Url, patterns : Patterns, strategyForComponentsforAWebsite : Strategies, dataModel : type):\n",
    "    dataModelComponents = list()\n",
    "    components = componentsOf(dataModel)\n",
    "    for component in components:\n",
    "        dataModelComponents += [strategyForComponentsforAWebsite[component](url, patterns[component])]\n",
    "    return dataModel(*dataModelComponents)\n",
    "\n",
    "# def scrapeWebsiteForTitleAndBody(url, patterns : Patterns, strategies : Strategies):\n",
    "\n",
    "#    title = strategies[\"title\"](url, patterns[\"title\"])\n",
    "#    body = strategies['body'](url, patterns['body'])\n",
    "\n",
    "#    return Content(url, title, body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on the classes to be used create the function determining what data to look for on the websites!\n",
    "#TODO:Do the application of the generalization if the generalization is done above\n",
    "scrapeWebsiteForTitleAndBody = partial(scrapeDomainForComponentsOfDataModel, dataModel = Content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enumerate the strategies to choose from!\n",
    "\n",
    "def searchForTextOfFirstMatch(url : Url, pattern : Pattern):\n",
    "    bs = getPage(url)\n",
    "    tag = getTextOfFirstTagMatchedInSoup(pattern, bs)\n",
    "    return tag\n",
    "\n",
    "def searchForTagByParts(url, pattern : Pattern):\n",
    "    tags = getPage(url).find_all(**pattern)\n",
    "    goalTag = ''\n",
    "    for tag in tags:\n",
    "        goalTag += (tag.text) + \"\\n\"\n",
    "    return goalTag\n",
    "\n",
    "def justReturnTheUrl(url, pattern : Pattern):\n",
    "    return url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the websites!\n",
    "domains = [\"brookings.edu\", \"nytimes.com\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the strategies to use for the websites!\n",
    "BrookingsStrategy = {\n",
    "    \"url\"   : justReturnTheUrl,\n",
    "    \"title\" : searchForTextOfFirstMatch, \n",
    "    \"body\"  : searchForTagByParts\n",
    "    }\n",
    "\n",
    "NYTimesStrategy = {\n",
    "    \"url\"   : justReturnTheUrl,\n",
    "    \"title\" : searchForTextOfFirstMatch,\n",
    "    \"body\" : searchForTagByParts\n",
    "}\n",
    "strategies = {\n",
    "    \"brookings.edu\"  : BrookingsStrategy,\n",
    "    \"nytimes.com\"   :NYTimesStrategy\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the patterns for the scrapers!\n",
    "\n",
    "BrookingsPatterns = {\n",
    "    \"url\"   : {},\n",
    "    \"title\" : {'name' : 'title'},\n",
    "    \"body\"  : {'name' : 'div', 'attrs' : {'class' : \"byo-block -narrow wysiwyg-block wysiwyg\"}}\n",
    "}\n",
    "\n",
    "NYTimesPatterns = {\n",
    "    \"url\"   : {},\n",
    "    \"title\" : {'name' : 'title'},\n",
    "    \"body\"  : {'selector' : ('div.StoryBodyCompanionColumn div p')}\n",
    "}\n",
    "domainPatterns = {\n",
    "    \"brookings.edu\" : BrookingsPatterns,\n",
    "    \"nytimes.com\"   : NYTimesPatterns\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Bill Baer's testimony before the U.S. Senate Committee on the Judiciary Subcommittee on Competition Policy, Antitrust, and Consumer Rights | Brookings\n",
      "URL: https://www.brookings.edu/articles/bill-baers-testimony-before-the-u-s-senate-committee-on-the-judiciary-subcommittee-on-competition-policy-antitrust-and-consumer-rights/\n",
      "\n",
      "\n",
      "Chair Klobuchar, Ranking Member Lee, and distinguished members of the Subcommittee, thank you for the opportunity to appear this afternoon and address one of the many challenges we face in harnessing the power and maximizing the potential of artificial intelligence.\n",
      "The growing use of pricing algorithms presents one such challenge. I am no expert in AI. But from the vantage point of this long-time antitrust enforcer, now just an antitrust worrier, there is good reason for concern that misuse of this tool is growing and puts consumers at risk of paying supracompetitive prices for all sorts of goods and services.1\n",
      "As your October hearing on the rental housing market explored, the potential misuse of pricing algorithms comes in many different forms. My testimony focuses on three collusive uses of AI pricing to harm competition and consumers: (1) head-to-head agreements between competitors to use the same pricing tools to fix prices; (2) hub and spoke agreements where competing firms use the same third partyâ€™s pricing algorithm to achieve anticompetitive outcomes; and (3) situations where widespread use of pricing algorithms by competitors may facilitate tacit collusion and cause significant consumer harms.\n",
      "Download the full testimony\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Define the scrapers for all websites to be parsed using the known data to look for, the patterns, and the strategies!\n",
    "from functools import partial\n",
    "\n",
    "scrapers = {\n",
    "    \"brookings.edu\" : partial(scrapeDomainForComponentsOfDataModel, patterns = BrookingsPatterns,  strategyForComponentsforAWebsite = BrookingsStrategy, dataModel = Content),\n",
    "    \"nytimes.com\"   : partial(scrapeDomainForComponentsOfDataModel, patterns = NYTimesPatterns,    strategyForComponentsforAWebsite = NYTimesStrategy, dataModel = Content)\n",
    "}\n",
    "print(scrapers[\"brookings.edu\"](url=\"https://www.brookings.edu/articles/bill-baers-testimony-before-the-u-s-senate-committee-on-the-judiciary-subcommittee-on-competition-policy-antitrust-and-consumer-rights/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a regex pattern for finding the domain of a website!\n",
    "import re\n",
    "def findDomainOf(url):\n",
    "    match = re.search(r'https?:\\/\\/(?:www\\.)?([^\\/:]+)', url)\n",
    "    if match:\n",
    "        domain = match.group(1)\n",
    "        return domain \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "scrapeDomainForComponentsOfDataModel() got an unexpected keyword argument 'strategies'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-922e6b3e7e0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#scrapeWebsiteForTitleAndBody = partial(scrapeUrlBasedOnASingleDataModelAndStrategiesAndPatternsForWebsites, dataModel = Content, strategies = strategies, patterns = patterns, domains = domains)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscrapeWebsiteForTitleAndBody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://www.brookings.edu/blog/future-development/2018/01/26/delivering-inclusive-urban-access-3-uncomfortable-truths/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatterns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdomainPatterns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrategies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#TODO: Find the website from the link!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: scrapeDomainForComponentsOfDataModel() got an unexpected keyword argument 'strategies'"
     ]
    }
   ],
   "source": [
    "#Define printing of url's based on their website \n",
    "#TODO:Do the generalization for the given inputs!\n",
    "# def scrapeUrlBasedOnASingleDataModelAndStrategiesAndPatternsForWebsites(url : Url, dataModel : type, strategies : Strategies, patterns : Patterns, domains : List[Url]):\n",
    "#     scrapeWebsiteForTheDataModel = partial(scrapeWebsiteForComponentsOfDataModel, dataModel = dataModel)\n",
    "#     scrapers = {domain : partial(scrapeWebsiteForTheDataModel, patterns = patterns[domain], strategies = strategies[domain]) for domain in domains}\n",
    "#     domain = findDomainOf(url)\n",
    "#     return scrapers[domain](url)\n",
    "\n",
    "#scrapeWebsiteForTitleAndBody = partial(scrapeUrlBasedOnASingleDataModelAndStrategiesAndPatternsForWebsites, dataModel = Content, strategies = strategies, patterns = patterns, domains = domains)\n",
    "print(scrapeWebsiteForTitleAndBody('https://www.brookings.edu/blog/future-development/2018/01/26/delivering-inclusive-urban-access-3-uncomfortable-truths/', patterns = domainPatterns, strategies = strategies))\n",
    "\n",
    "#TODO: Find the website from the link!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'findDomainOf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-bbef6aa131b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint_the_content_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://www.brookings.edu/blog/future-development/2018/01/26/delivering-inclusive-urban-access-3-uncomfortable-truths/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint_the_content_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.brookings.edu/articles/the-hamilton-project-2023-in-figures/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint_the_content_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.brookings.edu/articles/bill-baers-testimony-before-the-u-s-senate-committee-on-the-judiciary-subcommittee-on-competition-policy-antitrust-and-consumer-rights/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-107-bbef6aa131b1>\u001b[0m in \u001b[0;36mprint_the_content_of\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_the_content_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScrapeTitleAndBodyOf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-107-bbef6aa131b1>\u001b[0m in \u001b[0;36mScrapeTitleAndBodyOf\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Print the contents!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mScrapeTitleAndBodyOf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdomain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindDomainOf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscrapers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'findDomainOf' is not defined"
     ]
    }
   ],
   "source": [
    "#Print the contents!\n",
    "def ScrapeTitleAndBodyOf(url):\n",
    "    domain = findDomainOf(url)\n",
    "    return scrapers[domain](url)\n",
    "\n",
    "def print_the_content_of(url):\n",
    "    data = ScrapeTitleAndBodyOf(url)\n",
    "    print(data)\n",
    "\n",
    "print_the_content_of('https://www.brookings.edu/blog/future-development/2018/01/26/delivering-inclusive-urban-access-3-uncomfortable-truths/')\n",
    "print_the_content_of(\"https://www.brookings.edu/articles/the-hamilton-project-2023-in-figures/\")\n",
    "print_the_content_of(\"https://www.brookings.edu/articles/bill-baers-testimony-before-the-u-s-senate-committee-on-the-judiciary-subcommittee-on-competition-policy-antitrust-and-consumer-rights/\")\n",
    "print_the_content_of('https://www.nytimes.com/2018/01/25/opinion/sunday/silicon-valley-immortality.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing scraper.\n",
      "Data model's components: ['url', 'title', 'body']\n",
      "Domain strategies are: {'brookings.edu': {'url': {}, 'title': {'name': 'title'}, 'body': {'name': 'div', 'attrs': {'class': 'byo-block -narrow wysiwyg-block wysiwyg'}}}, 'nytimes.com': {'url': {}, 'title': {'name': 'title'}, 'body': {'selector': 'div.StoryBodyCompanionColumn div p'}}}\n",
      "Title: Bill Baer's testimony before the U.S. Senate Committee on the Judiciary Subcommittee on Competition Policy, Antitrust, and Consumer Rights | Brookings\n",
      "URL: https://www.brookings.edu/articles/bill-baers-testimony-before-the-u-s-senate-committee-on-the-judiciary-subcommittee-on-competition-policy-antitrust-and-consumer-rights/\n",
      "\n",
      "\n",
      "Chair Klobuchar, Ranking Member Lee, and distinguished members of the Subcommittee, thank you for the opportunity to appear this afternoon and address one of the many challenges we face in harnessing the power and maximizing the potential of artificial intelligence.\n",
      "The growing use of pricing algorithms presents one such challenge. I am no expert in AI. But from the vantage point of this long-time antitrust enforcer, now just an antitrust worrier, there is good reason for concern that misuse of this tool is growing and puts consumers at risk of paying supracompetitive prices for all sorts of goods and services.1\n",
      "As your October hearing on the rental housing market explored, the potential misuse of pricing algorithms comes in many different forms. My testimony focuses on three collusive uses of AI pricing to harm competition and consumers: (1) head-to-head agreements between competitors to use the same pricing tools to fix prices; (2) hub and spoke agreements where competing firms use the same third partyâ€™s pricing algorithm to achieve anticompetitive outcomes; and (3) situations where widespread use of pricing algorithms by competitors may facilitate tacit collusion and cause significant consumer harms.\n",
      "Download the full testimony\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Make the following code work\n",
    "\n",
    "#Define the scrapers for all websites to be parsed using the known data to look for, the patterns, and the strategies!\n",
    "from functools import partial\n",
    "import re\n",
    "from typing import Callable, Tuple, Dict, List\n",
    "\n",
    "class SingleDataModelScraper:\n",
    "    Pattern = dict\n",
    "    Url = str\n",
    "    Title = str \n",
    "    Body = str\n",
    "    Strategy = Callable\n",
    "    Strategies = Dict[str, Strategy]\n",
    "    Patterns = Dict[str, Pattern]\n",
    "    Component = str\n",
    "    Domain = str\n",
    "\n",
    "    def componentsOf(dataModel : type): #TODO: Test whether base class has default value or not\n",
    "    # if not issubclass(dataModel, TypeWithADefaultValue):\n",
    "    #     raise TypeError(\"The dataModel does not have a default value.\")\n",
    "        return list(dataModel().__dict__.keys())\n",
    "\n",
    "    def scrapeDomainForComponentsOfDataModel(url : Url, patterns : Patterns, strategyForComponentsforADomain : Strategies, dataModel : type):\n",
    "        dataModelComponents = list()\n",
    "        #Get the components of the data model!\n",
    "        components = componentsOf(dataModel)\n",
    "        #For each component of the data model, according to the strategy for the given website's component, compute the component's value!\n",
    "        for component in components:\n",
    "            dataModelComponents += [strategyForComponentsforADomain[component](url, patterns[component])]\n",
    "        #Return the instance of the data model from the components!\n",
    "        return dataModel(*dataModelComponents)\n",
    "    \n",
    "    def __init__(self, dataModel : type, domainStrategies : Strategies, domainPatterns : Patterns, domains : List[Domain] = []):\n",
    "        print(\"Initializing scraper.\")\n",
    "        print(f\"Data model's components: {componentsOf(dataModel)}\")\n",
    "        self.dataModel = dataModel\n",
    "        print(f\"Domain strategies are: {str(domainStrategies)}\")\n",
    "        print()\n",
    "        self.domainStrategies = domainStrategies\n",
    "        self.domainPatterns = domainPatterns\n",
    "        #For each domain, create a scraper for the domain that is capable of scraping any website of the domain and returning for each website an instance of the desired data model!\n",
    "        self.scrapers = {domain : partial(self.scrapeDomainForComponentsOfDataModel, \n",
    "                                          patterns = domainPatterns[domain], \n",
    "                                          strategyForComponentsforAWebsite = domainStrategies[domain], \n",
    "                                          dataModel = dataModel) for domain in domains}\n",
    "\n",
    "    def __findDomainOf(self, url):\n",
    "        match = re.search(r'https?:\\/\\/(?:www\\.)?([^\\/:]+)', url)\n",
    "        if match:\n",
    "            domain = match.group(1)\n",
    "            return domain \n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def __call__(self, url):\n",
    "        domain = self.__findDomainOf(url)\n",
    "        try:\n",
    "            return scrapers[domain](url)\n",
    "        except KeyError as k:\n",
    "            print(\"There does not exist a strategy for the given domain!\")\n",
    "\n",
    "    \n",
    "    def addWebsiteStrategy(self, domain, domainStrategy):\n",
    "        pass\n",
    "\n",
    "    def addToDomainStrategy(self, domain, component, strategyForComponent):\n",
    "        pass \n",
    "\n",
    "    def removeFromDataModel(self, component):\n",
    "        pass \n",
    "    def addToDataModel(self, component):\n",
    "        pass\n",
    "bndomains = ['brookings.edu', 'nytimes.com']\n",
    "scraper = SingleDataModelScraper(Content, domainPatterns, strategies, bndomains)\n",
    "print(scraper(\"https://www.brookings.edu/articles/bill-baers-testimony-before-the-u-s-senate-committee-on-the-judiciary-subcommittee-on-competition-policy-antitrust-and-consumer-rights/\"))\n",
    "\n",
    "#TODO: Ensure, that the class's dataModel has to have a method for printing it's elements!\n",
    "#scraper.modifyStrategy(website, dataComponent, newStrategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BrookingsPatterns = {\n",
    "    \"url\"   : {},\n",
    "    \"title\" : {'name' : 'title'},\n",
    "    \"body\"  : {'name' : 'div', 'attrs' : {'class' : \"byo-block -narrow wysiwyg-block wysiwyg\"}}\n",
    "}\n",
    "\n",
    "NYTimesPatterns = {\n",
    "    \"url\"   : {},\n",
    "    \"title\" : {'name' : 'title'},\n",
    "    \"body\"  : {'selector' : ('div.StoryBodyCompanionColumn div p')}\n",
    "}\n",
    "domainPatterns = {\n",
    "    \"brookings.edu\" : BrookingsPatterns,\n",
    "    \"nytimes.com\"   : NYTimesPatterns\n",
    "}\n",
    "\n",
    "#Define the strategies to use for the websites!\n",
    "BrookingsStrategy = {\n",
    "    \"url\"   : justReturnTheUrl,\n",
    "    \"title\" : searchForTextOfFirstMatch, \n",
    "    \"body\"  : searchForTagByParts\n",
    "    }\n",
    "\n",
    "NYTimesStrategy = {\n",
    "    \"url\"   : justReturnTheUrl,\n",
    "    \"title\" : searchForTextOfFirstMatch,\n",
    "    \"body\" : searchForTagByParts\n",
    "}\n",
    "domainStrategies = {\n",
    "    \"brookings.edu\"  : BrookingsStrategy,\n",
    "    \"nytimes.com\"   :NYTimesStrategy\n",
    "}\n",
    "\n",
    "domains = [\"brookings.edu\", \"nytimes.com\"]\n",
    "\n",
    "scraper = SingleDataModelScraper(Content, domainPatterns, domainStrategies, domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \tSome name of a listing\n",
      "Price: \t50 dollars\n",
      "Parameters: Label: Color, Value: ['B', 'l', 'a', 'c', 'k']\n",
      "\n",
      "There does not exist a strategy for the given domain!\n"
     ]
    }
   ],
   "source": [
    "domains = [\"ebay.com\"]\n",
    "\n",
    "class Parameter:\n",
    "    def __init__(self, label, values):\n",
    "        self.label = label \n",
    "        self.values = values\n",
    "    def __str__(self):\n",
    "        return f\"Label: {str(self.label)}\" + \", \" + f\"Value: {[value for value in self.values]}\" + \"\\n\"\n",
    "\n",
    "class Listing:\n",
    "    def __init__(self, name, price, parameters : List[Parameter]):\n",
    "        self.name = name \n",
    "        self.price = price \n",
    "        self.parameters = parameters \n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Name: \\t{str(self.name)}\" + \"\\n\" + f\"Price: \\t{self.price}\" + \"\\n\" + f\"Parameters: {[str(parameter) for parameter in self.parameters][0]}\"\n",
    "parameters = [Parameter(\"Color\", \"Black\"), Parameter(\"Color\", \"White\")]\n",
    "listing = Listing(\"Some name of a listing\", \"50 dollars\", parameters)\n",
    "print(listing)\n",
    "\n",
    "#Specific strategies\n",
    "def parameterTagToParameter(parameterTag):\n",
    "    label = parameterTag.label.span.span.text\n",
    "    values = []\n",
    "    listOfValueTags = parameterTag.find({\"name\" : \"\", \n",
    "                            \"attrs\" : \n",
    "                                {\n",
    "                                    \"class\" : \"x-msku__select-box\"\n",
    "                                }})\n",
    "    for valueTag in listOfValueTags:\n",
    "        values.append(valueTag.text)\n",
    "    return Parameter(label, values)\n",
    "\n",
    "def getListAndTransformItsElementsToParameters(url, patterns):\n",
    "    bs = getPage(url)\n",
    "    parameters = bs.find(patterns[\"listpattern\"])\n",
    "    for parameterTag in parameters:\n",
    "        parameter = parameterTagToParameter(parameterTag)\n",
    "\n",
    "ebayStrategy = {\n",
    "    \"name\"  : searchForTextOfFirstMatch,\n",
    "    \"price\" : searchForTextOfFirstMatch,\n",
    "    \"parameters\" : getListAndTransformItsElementsToParameters\n",
    "}\n",
    "ebayPatterns = {\n",
    "    \"name\"       : {\"name\" : \"h1\", \"attrs\" : {\"class\" : \"x-item-title__mainTitle\"}},\n",
    "    \"price\"      : {\"name\" : \"span\", \"attrs\" : {\"class\" : \"ux-textspans\"}},\n",
    "    \"parameters\" : {\"listpattern\" : {\"attrs\" : {\"class\" : \"vim x-msku mar-t-16\", \"data-testid\" : \"x-msku\"}}},\n",
    "}\n",
    "\n",
    "domainStrategies2 = {\"ebay.com\" : ebayStrategy}\n",
    "domainPatterns2 = {\"ebay.com\" : ebayPatterns}\n",
    "\n",
    "scraper2 = SingleDataModelScraper(Listing, \n",
    "                       domainPatterns = domainPatterns2, \n",
    "                       domainStrategies = domainStrategies2, \n",
    "                       domains = [\"ebay.com\"])\n",
    "\n",
    "scraper2(\"https://www.ebay.com/itm/175900855483?hash=item28f4820cbb:g:~HgAAOSwYZxlAcv2&amdata=enc%3AAQAIAAAA4J0jJ3Q5dqoGZEAodn%2B8vXjyi8HrMbuCrwdVPoFbx%2FBbTAOmMlxlh8cAXWBqN4C%2FgqIICnj%2Flm3OJOlaMNZCxjFsamek%2FDr0dW3iI890bPZD9OGXyACjohCI%2FvQsWfSYtXKnhoHM5dOh0s3D5mpn0FbeecvixKUzQbhbS%2FREAt1Ie9ZvXsWxJx61x0rWbo6ro4C9nw65fleMVLTLcxWAjACeMm967kwdygPnDkS%2Ft%2FmWKkOiJ5iKM9cJHH276GFWhnc7ac3BuPaCkhysP3%2Bp%2F28v802Ln9CXGn8jkSDH1alA%7Ctkp%3ABFBM0LqwuJBj&var=475333376890\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
